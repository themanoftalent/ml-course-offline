<section>
  <h2>Attention Mechanisms (Transformers)</h2>
  <div class="wrapper">
    <div class="left">
      <svg><!-- Insert SVG here --></svg>
    </div>
    <div class="right">
      <p class="body-text">
        Attention Mechanisms, also known as Transformers, are a type of neural
        network architecture that is particularly well-suited for natural
        language processing tasks. The key difference between Attention
        Mechanisms and traditional feedforward neural networks is the use of
        attention mechanisms, which allow the network to focus on specific parts
        of the input data. Attention mechanisms work by assigning a weight to
        each element of the input data, allowing the network to focus more on
      </p>
    </div>
  </div>
</section>

<style>
  h2 {
    font-size: var(--size-default);
    text-decoration: underline;
  }
  .wrapper {
    display: grid;
    grid-template-columns: 30% 70%;
    grid-gap: 1rem;
    align-items: center;
    max-width: 100%;
    margin: auto;
  }
  .left {
    display: flex;
    justify-content: center;
  }
  .right {
    /* border: 2px solid black; */
  }
  svg {
    max-width: 100%;
    max-height: 100%;
    border: 2px solid red;
  }
</style>
